# -*- coding: utf-8 -*-
"""python_task_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OzP56a-mjnJPvDWN9Ok9HU6ZXNsYJyJ5

# QUESTION NO : 1
# Car Matrix Generation
"""

import pandas as pd

def generate_car_matrix(dataset_path):
    # Read the dataset
    dataset = pd.read_csv(dataset_path)

    # Create a pivot table using id_1, id_2, and car columns
    pivot_table = dataset.pivot_table(index='id_1', columns='id_2', values='car', fill_value=0)

    # Ensure diagonal values are 0
    for i in range(min(pivot_table.shape[0], pivot_table.shape[1])):
        pivot_table.iloc[i, i] = 0

    return pivot_table

# Specify the path to the uploaded dataset
dataset_path_colab = '/content/dataset-1.csv'

# Example usage
result_matrix = generate_car_matrix(dataset_path_colab)

# Print the result
print(result_matrix)

"""# QUESTION NO: 2
# Car Type Count Calculation

"""

import pandas as pd
import numpy as np

def get_type_count(dataset):
    # Read the dataset if not already a DataFrame
    if not isinstance(dataset, pd.DataFrame):
        dataset = pd.read_csv(dataset)

    # Print the original dataset
    print("Original Dataset:")
    print(dataset)

    # Add a new categorical column 'car_type' based on 'car' values
    conditions = [
        (dataset['car'] <= 15),
        (dataset['car'] > 15) & (dataset['car'] <= 25),
        (dataset['car'] > 25)
    ]
    choices = ['low', 'medium', 'high']
    dataset['car_type'] = np.select(conditions, choices, default='unknown')

    # Print the dataset with the new 'car_type' column
    print("\nDataset with 'car_type' column:")
    print(dataset)

    # Calculate the count of occurrences for each 'car_type' category
    type_counts = dataset['car_type'].value_counts().to_dict()

    # Sort the dictionary alphabetically based on keys
    type_counts_sorted = {key: type_counts[key] for key in sorted(type_counts)}

    # Print the sorted dictionary
    print("\nType Counts (Sorted Alphabetically):")
    print(type_counts_sorted)

    return type_counts_sorted

# Example usage
dataset_path = '/content/dataset-1.csv'
result_type_count = get_type_count(dataset_path)

# Print the final result
print("\nFinal Result:")
print(result_type_count)

"""# QUESTION NO: 3
# Bus Count Index Retrieval
"""

import pandas as pd
import numpy as np

def get_bus_indexes(dataset):
    # Read the dataset if not already a DataFrame
    if not isinstance(dataset, pd.DataFrame):
        dataset = pd.read_csv(dataset)

    # Print the original dataset
    print("Original Dataset:")
    print(dataset)

    # Calculate the mean value of the 'bus' column
    mean_bus = dataset['bus'].mean()

    # Print the mean value of the 'bus' column
    print("\nMean Value of 'bus' Column:", mean_bus)

    # Identify indices where 'bus' values are greater than twice the mean
    bus_indexes = dataset[dataset['bus'] > 2 * mean_bus].index.tolist()

    # Print the indices where 'bus' values are greater than twice the mean
    print("\nIndices where 'bus' values are greater than twice the mean:")
    print(bus_indexes)

    # Sort the indices in ascending order
    bus_indexes_sorted = sorted(bus_indexes)

    # Print the sorted indices
    print("\nSorted Indices in Ascending Order:")
    print(bus_indexes_sorted)

    return bus_indexes_sorted

# Example usage
dataset_path = '/content/dataset-1.csv'
result_bus_indexes = get_bus_indexes(dataset_path)

# Print the final result
print("\nFinal Result:")
print(result_bus_indexes)

"""# QUESTION NO: 4
# Route Filtering
"""

import pandas as pd

def filter_routes(df):
    # Print the original DataFrame
    print("Original DataFrame:")
    print(df)

    # Convert 'truck' column to numeric, handle errors='coerce' to convert non-numeric values to NaN
    df['truck'] = pd.to_numeric(df['truck'], errors='coerce')

    # Print the DataFrame after converting 'truck' column
    print("\nDataFrame after converting 'truck' column to numeric:")
    print(df)

    # Filter routes based on average truck values greater than 7
    filtered_routes = df.groupby('route')['truck'].mean().loc[lambda x: x > 7].index.tolist()

    # Print the list of routes with average truck values greater than 7
    print("\nRoutes with average 'truck' values greater than 7:")
    print(filtered_routes)

    # Sort the list of routes
    sorted_filtered_routes = sorted(filtered_routes)

    # Print the final result
    print("\nSorted list of routes with average 'truck' values greater than 7:")
    print(sorted_filtered_routes)

    return sorted_filtered_routes

# Example usage
# Assuming df is the DataFrame from dataset-1.csv
df = pd.read_csv('/content/dataset-1.csv')  # Uncomment and replace with the actual path if reading from a file

result_routes = filter_routes(df)

"""# QUESTION NO:5
# Matrix Value Modification
"""

import pandas as pd

def multiply_matrix(input_matrix):
    # Make a copy of the input matrix to avoid modifying the original
    modified_matrix = input_matrix.copy()

    # Apply the specified logic to each value in the DataFrame
    for i in range(modified_matrix.shape[0]):
        for j in range(modified_matrix.shape[1]):
            if modified_matrix.iloc[i, j] > 20:
                modified_matrix.iloc[i, j] *= 0.75
            else:
                modified_matrix.iloc[i, j] *= 1.25

    # Round the values to 1 decimal place
    modified_matrix = modified_matrix.round(1)

    return modified_matrix

# Example usage
# Assuming result_matrix is the DataFrame from Question 1
result_matrix_modified = multiply_matrix(result_matrix)

# Print the modified result
print("Original DataFrame:")
print(result_matrix)
print("\nModified DataFrame:")
print(result_matrix_modified)

"""# QUESTION NO: 6
# Time Check
"""

import pandas as pd

def check_time_completeness(df):
    # Check for missing values in timestamp columns
    if df[['startDay', 'startTime', 'endDay', 'endTime']].isnull().any().any():
        print("Error: Missing values in timestamp columns.")
        return None

    try:
        # Convert start and end timestamps to datetime objects
        df['start_datetime'] = pd.to_datetime(df['startDay'] + ' ' + df['startTime'])
        df['end_datetime'] = pd.to_datetime(df['endDay'] + ' ' + df['endTime'])
    except pd.errors.OutOfBoundsDatetime as e:
        print(f"Error: {e}")
        return None

    # Check if timestamps cover a full 24-hour period
    full_24_hours = (df['end_datetime'] - df['start_datetime']).dt.total_seconds() == 24 * 3600

    # Check if timestamps span all 7 days of the week
    all_days_present = df.groupby(['id', 'id_2'])['start_datetime'].apply(lambda x: x.dt.dayofweek.nunique() == 7)

    # Combine both conditions
    completeness_series = full_24_hours & all_days_present

    return completeness_series

# Example usage
dataset_path = '/content/dataset-2.csv'
df_dataset_2 = pd.read_csv(dataset_path)

# Check for potential issues before proceeding
if df_dataset_2[['startDay', 'startTime', 'endDay', 'endTime']].isnull().any().any():
    print("Error: Missing values in the timestamp columns.")
else:
    result_completeness = check_time_completeness(df_dataset_2)
    print(result_completeness)